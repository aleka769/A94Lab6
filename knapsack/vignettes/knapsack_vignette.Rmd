---
title: "Intro to knapsack package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Intro to knapsack}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(profvis)
library(parallel)
library(testthat)
```

`knapsack` allows you to pack your knapsack in the optimal way, without using your brain! If you have too many options, or if you're just lazy, then let the computer think for you... Included are no less than three different algorithms that optimizes the value of the stuff you want to pack without extending the limitaions of your knapsack:

* `brute_force_knapsack` -- If you're not afraid of putting your computer to the limit, use this method!

* `dynamic_knapsack` -- A little more gentle to memory, yet as precise as the brute force route!

* `greedy_knapsack` -- You're not a perfectionist, but you still want to pack like a pro? Well, go ahead and use the heuristic approach!

To give you a nice intro for the mentioned functions, look below. 

**Pack on!!**

## The three knapsack approaches 101

All approaches to optimization are presented with code below. 

### Brute force: search through all combinations of items
The simplest implementation simply tests all combinations of items and returns the one with the highest value. The method is both time and resource consuming and will take $0.124 s$ to run for 16 objects on the test-laptop with an Intel i7-5500.

```{r, echo = TRUE, eval = FALSE, message = FALSE}
brute_force_knapsack(x = knapsack_objects[1:8,], W = 3500)
```

### Dynamic: 
The dynamic approach is faster an less resource hungry than the brute force version, and is still guaranteed to return the optimal result. The function takes $0.025s$ to run on the test-laptop for 500 objects.

```{r, echo = TRUE, eval = FALSE, message = FALSE}
dynamic_knapsack(x = knapsack_objects[1:8,], W = 3500)
```

### Greedy: pack the most value-dense object firsts
The last implementation simply calculates the value to weight ratio of all objects and packs the best ones it can without overriding the weight limit. Running this algorithm for 1 million objects takes roughtly $14 s$ to run on the test-laptop. Since the only calculation done is the value-weight ratio, (which has the same $O(n)$ as the later while loop) and the sorting of this ratio vector takes more time than these, the overall complexity of the method is $O(n \cdot log(n))$.

```{r, echo = TRUE, eval = FALSE, message = FALSE}
greedy_knapsack(x = knapsack_objects[1:1000000], W = 3500)
```

## Profiling of code using `profvis`

The package `lineprof` is not available for R version 3.4.2, the package `profvis` is instead used for profiling. It is not possible to see details of the function contents when `profvis` is used as a wrapper for functions in the `knapsack` package. Therefore, another approach to profiling has been made and `echo` is set to `FALSE` in code chunks below. The code contents can be reviewed in output for functions below.

### Greedy heuristic function

```{r, echo = TRUE, eval = TRUE, message = FALSE}
# Sampled data for profiling
set.seed(42)
n <- 2000
knapsack_objects <- data.frame(
  w=sample(1:4000, size = n, replace = TRUE),
  v=runif(n = n, 0, 10000)
)

# Assign x and W to match function...
x <- knapsack_objects[1:1000,]
W <- 3500
```


```{r, echo = FALSE, eval = TRUE}
# Profiling of heuristic function:
p <- profvis::profvis({
  # pausing because function is too fast!
  profvis::pause(.01)
  
  stopifnot(is.data.frame(x))
  stopifnot(c("v", "w") %in% names(x))
  stopifnot(is.numeric(x$w)&&is.numeric(x$v))
  stopifnot(any(x$w <= W) || any(x$v > 0) )
  stopifnot(W > 0)

  x$quota <- x$v / x$w
  row_order <- order(x$quota, decreasing = TRUE)
  x <- x[row_order,]

  value <- 0
  elements <- vector("integer")
  cum_weight <- 0
  curr_object <- 1


  while (curr_object <= dim(x)[1]){
    if (cum_weight + x[curr_object,"w"] > W) {
      curr_object <- curr_object + 1
    }
    else {
      value <- value + x[curr_object,"v"]
      cum_weight <- cum_weight + x[curr_object,"w"]
      elements <- c(elements, curr_object)
      curr_object <- curr_object + 1
    }

  }

  list(value = value,
       elements = sort(row_order[elements]),
       cum_weight)
})

p
```

The computational time without the `pause` call is ($0.03 \pm 0.01$) seconds. The while-loop and if-statement are most time consuming. In order to speed the function up, an implementation in C++ would probably be appropriate. 

### Brute force function

```{r, echo = TRUE, eval = TRUE}
# Assign x, W and parallel to match function...
x        <- knapsack_objects[1:16,] 
W        <- 3500
parallel <- FALSE
```


```{r, echo = FALSE, eval = TRUE}
# Profiling of brute force function:
p <- profvis::profvis({
  # pausing because function is too fast!
  profvis::pause(.01)
  
  # error checks
  stopifnot(is.data.frame(x))
  stopifnot(c("v", "w") %in% names(x))
  stopifnot(is.numeric(x$w)&&is.numeric(x$v))
  stopifnot(any(x$w <= W) || any(x$v > 0))
  stopifnot(W > 0)
  
  n <- dim(x)[1]
  
  max_objects <- 30L
  min_parallel <- 20L
  force_parallel <- 25L
  # stopping if data will be too large
  stopifnot(n <= max_objects)
  if(.Platform$OS.type != "unix" & n > force_parallel){
    stop("Can only run calculations on large object sets in parallel which requires Linux")
  }
  
  # define function to do actual computaion and generate candidates for max value
  max_core_combo <- function(index, combo_vec=core_combos[[index]], object_matrix){
    n <- dim(object_matrix)[1]
    
    combn_mat <- matrix(NA, nrow = length(combo_vec), ncol = n)
    for (rownum in  1:length(combo_vec)){
      combn_mat[rownum, ] <- as.integer(intToBits(combo_vec[rownum]))[1:n]
    }
    
    result_mat <- combn_mat %*% object_matrix
    
    # removing combinations with weight above maxweight
    result_mat[result_mat[,"w"] > W, ] <- NA
    
    max_element <- which.max(result_mat[,"v"])
    
    c(unname(result_mat[max_element, ]), combn_mat[max_element, ])
  }
  
  # checking if parallelization is requested and can be used
  # workaround to get parallelisation through check according to
  # https://cran.r-project.org/web/packages/policies.html
  chk <- Sys.getenv("_R_CHECK_LIMIT_CORES_", "")
  
  if (nzchar(chk) && chk == "TRUE") {
    # use 2 cores in CRAN/Travis/AppVeyor
    core_count <- 2L
  }
  else if (n >= min_parallel & parallel & .Platform$OS.type == "unix"){ 
    core_count <- parallel::detectCores()-1 
  }
  else if (parallel & .Platform$OS.type != "unix") {
    core_count <- 1L 
    message("Can't parallelize on non-unix OS, using one core!")
  }
  else {
    core_count <- 1L
  }
  
  # experimental way of handling memory issues in large object sets
  if (n %in% seq(force_parallel,max_objects)) {
    core_count <- core_count * 2^(n-force_parallel)
  }
  
  # generate list of numbers for each core to copmute on
  combo_count <- (2^n)-1L
  combos_per_core <- combo_count%/%core_count
  core_combos <- lapply(1:core_count, function(x){
    seq.int(from=as.integer((x-1)*(combos_per_core)),
            to=as.integer((x)*(combos_per_core)),
            by=1L)
  })
  if (combo_count%%core_count != 0){
    remainder_seq <- seq(combos_per_core*core_count+1,combo_count,1)
    core_combos[[core_count]]<-c(core_combos[[core_count]],remainder_seq)
  }
  
  # runs computation in the specified no of segments and cores
  x <- as.matrix(x[,c("v","w")])
  max_per_core <-  parallel::mclapply(X = 1:core_count, FUN = max_core_combo, 
                                      object_matrix = x, mc.cores = core_count)
  max_element <- which.max(do.call(rbind, max_per_core)[,1])
  
  list(value = max_per_core[[max_element]][1],
       elements = seq(1,n,1)[max_per_core[[max_element]][-c(1,2)] == 1L])
})

p
```

The time demanding steps above is not suprisingly `intToBits` and `mcLapply`. However, the `profvis` profiling has been done with a windows OS using only one core, thereby computational time can probably be lowered if more cores would be used. A small amout of time was cut down by changing from `seq()` to `seq.int()` in the lapply call, but for the aqtual workhorses `intToBits` and `mcLapply`, we haven't found a way to cut down time. Perhaps an `Rcpp`-implementation of creating the binary matrix would speed things up, but this is not something we have tried to implement.

## Parallelisation

The brute force function has an option to run in parallel. This utilizes `parallel::mclapply()` and will hence not be honored under non-unix operating systems (windows will run a single-core calculation). When set to `TRUE`, the function wil split the binary matrix representing possible object combinations into a number of chunks equal to the number of available CPU cores - 1 as returned by `parallel::detectCores()`. In the event that the number of objects that are sent to the function are more than 25, parallelisation will be forced, both to speed up the computation and to save memory. with fewer than 20 bjects, the overhead of parallelisation is large enough to not give a boost and hence will run serial instead.

Performance scales roughtly linearly with the number of cores available for computation, especially for larger number of objects (that is running on 4 cores almost cuts the time needed down to 25% of running on one core).

More than 30 objects will stop the function since this requires more memory than generally found in standard PC's.
